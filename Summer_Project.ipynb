{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arvindd22/Automated-Research-Article-Analysis-System/blob/main/Summer_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Automated Research Articles Analysis System\n",
        "- This project helps us to classify the relevant research articles based on presence of relevant graphs, X-Y axis labels and input keywords.\n",
        "- We have used Pytesseract library to extract text from the images found in\n",
        "research articles.\n",
        "- Trained a Yolov8 model on shape memory alloy graphs data, to help us to identify graphs present in images of research articles.\n",
        "- Below is the complete code implementation of the project."
      ],
      "metadata": {
        "id": "RT1ljHOnkVKk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6SW4srwZmyb"
      },
      "source": [
        "# Connecting the Drive to the Notebook\n",
        "* Running this cell will ask for permission to mount the google drive to save the files on your drive it is neccesary to proceed further\n",
        "* All files downloaded throughout the notebook will be accessed to Google Drive >> My Drive  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p20uN1k4ovh2",
        "outputId": "f303b6dd-3aa8-455c-8fd2-3c69bf859a1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Scopus.csv file and put it in MyDrive folder of your Google Drive\n"
      ],
      "metadata": {
        "id": "GlUThuWHrAeD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPfZeYDLZucW"
      },
      "source": [
        "# Loading The CSV file\n",
        "- Using the pandas library in python we will load the csv file to dataframe 'df' and further create a dataframe 'dois' having only DOI column of entered scopus csv.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxYJeJXDpOR-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "csv = pd.read_csv('/content/drive/MyDrive/scopus.csv')\n",
        "df = pd.DataFrame(csv)\n",
        "dois =df['DOI'].astype(str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ubuFLdTZ7CU"
      },
      "source": [
        "# Downloading the XMLs from Scopus using SCOPUS API\n",
        "\n",
        "1. Library Imports:\n",
        "   The code begins by importing necessary Python libraries: 'os' for file system operations, 'requests' for making HTTP requests, and 'pandas'.\n",
        "\n",
        "2. Initialization:\n",
        "   - An empty list 'status_code' is created to store HTTP status codes.\n",
        "   - A counter 'j' is initialized to 1, likely for file naming purposes.\n",
        "   - A directory '/content/drive/MyDrive/xmls' is created using os.makedirs() with the 'exist_ok=True' parameter, ensuring the operation doesn't raise an error if the directory already exists.\n",
        "\n",
        "3. Main Processing Loop:\n",
        "   The code iterates through a list of Digital Object Identifiers (DOIs) stored in the 'dois' variable (which is not defined in the provided snippet but is assumed to exist).\n",
        "\n",
        "   For each DOI:\n",
        "   a. An XML URL is constructed using the Elsevier API endpoint and the current DOI.\n",
        "   b. HTTP headers are set, including an API key for authentication and specifying XML as the accepted response format.\n",
        "   c. A GET request is made to the constructed URL using the requests library, with streaming enabled and a 30-second timeout.\n",
        "   d. The HTTP status code of the response is appended to the 'status_code' list.\n",
        "\n",
        "   e. If the status code is 200 (indicating a successful request):\n",
        "      - The code attempts to open a new file in binary write mode, named numerically (1.xml, 2.xml, etc.) in the previously created directory.\n",
        "      - It then writes the response content to this file in chunks of 2048 bytes.\n",
        "      - If an exception occurs during this process, it's silently caught and the loop continues to the next iteration.\n",
        "\n",
        "   f. The counter 'j' is incremented, regardless of whether the file write was successful.\n",
        "\n",
        "4. Data Storage:\n",
        "   - The collected status codes are added as a new column 'StatusCode' to a pre-existing DataFrame 'df'.\n",
        "   - This DataFrame is then saved as a CSV file named 'scopus.csv' in the '/content/drive/MyDrive/' directory.\n",
        "\n",
        "In summary, we retrieve XML data for a list of DOIs from the Elsevier API, save successful responses as individual XML files, track the HTTP status codes of all requests, and finally saves these status codes along with other (scopus.csv) data in a CSV file.\n",
        "\n",
        "##Requirements : Enter the Elsevier API key to download the XML files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT0WIsAWptgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f299c0a-e87d-4010-c2bc-563503d5a3c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-b66cb8da8f55>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['StatusCode'] = status_code\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "status_code = []\n",
        "j =1\n",
        "os.makedirs('/content/drive/MyDrive/xmls',exist_ok = True)\n",
        "for i, doi in enumerate(dois):\n",
        "  xml_url='https://api.elsevier.com/content/article/doi/' + doi + '?view=FULL'\n",
        "\n",
        "  headers = {\n",
        "            'X-ELS-APIKEY': 'c8cf76a7333ef42cbb26d1ce3aadc8bd', # ENTER YOUR OWN API KEY\n",
        "            'Accept': 'text/xml'\n",
        "          }\n",
        "\n",
        "  r = requests.get(xml_url,stream = True,  headers=headers, timeout=30)\n",
        "\n",
        "  status_code.append(r.status_code)\n",
        "  if r.status_code == 200:\n",
        "    try:\n",
        "      writefile  = open(\"/content/drive/MyDrive/xmls/\" + str(j) + \".xml\", 'wb')\n",
        "\n",
        "\n",
        "\n",
        "      for chunk in r.iter_content(2048):\n",
        "        writefile.write((chunk))\n",
        "    except:\n",
        "      continue\n",
        "  j = j+1\n",
        "\n",
        "df['StatusCode'] = status_code\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/scopus.csv',index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d-JuqelaOlE"
      },
      "source": [
        "\n",
        "# Code Extracts captions and image information from XML files and writes them to CSV files.\n",
        "#### Using Element Tree to search the XML file\n",
        "\n",
        "Import libraries:\n",
        "\n",
        "**xml.etree.ElementTree**\n",
        "\n",
        "This library is used for parsing XML files.\n",
        "\n",
        "**csv**\n",
        "\n",
        "This library is used for reading and writing CSV files.\n",
        "\n",
        "**re**\n",
        "\n",
        "This library is used for regular expressions (used for cleaning captions).\n",
        "\n",
        "Loop through each row in a pandas dataframe (df):\n",
        "\n",
        "Check if the 'StatusCode' column value is 200 (indicating success).\n",
        "\n",
        "If the status code is 200:\n",
        "Get the file path for the corresponding XML file based on the row index.\n",
        "\n",
        "Try parsing the XML file:\n",
        "If parsing is successful:\n",
        "Initialize empty lists for images and captions.\n",
        "\n",
        "Loop through all 'figure' elements in the XML file:\n",
        "Extract the 'locator' attribute from the 'link' element within the 'figure' element (if it exists).\n",
        "Extract the caption text from the 'simple-para' element within the 'caption' element (if it exists).\n",
        "Clean the caption text using regular expressions (remove HTML tags and extra spaces).\n",
        "Append the caption text (or None if not found) to the 'captions' list.\n",
        "Loop through all 'attachment' elements in the XML file:\n",
        "Extract 'attachment_eid', 'ucs_locator', and 'filename' from the element and its child elements.\n",
        "Append a tuple containing this information to the 'images' list.\n",
        "\n",
        "Create a new CSV file with headers 'UTD EID', 'UCS Locator', 'Filename', and 'Caption'.\n",
        "Write each image information (along with its corresponding caption) to the CSV file.\n",
        "Read the created CSV file into a pandas dataframe.\n",
        "Add a new column 'link' to the dataframe. This column contains URLs for the images based on their 'UTD EID' values.\n",
        "\n",
        "Save the updated dataframe back to the CSV file.\n",
        "Except if there's an XML parsing error:\n",
        "Print an informative message indicating the error and the file that caused it.\n",
        "Except for any other errors:\n",
        "Print a generic error message mentioning the file that caused the error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBs7SjVLqBR4",
        "outputId": "4a6a0a55-94c4-4c66-b457-121a437a045b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "4\n",
            "6\n",
            "10\n",
            "14\n",
            "29\n",
            "38\n",
            "ParseError: no element found: line 1, column 0. The file /content/drive/MyDrive/xmls/38.xml might be incomplete or malformed.\n"
          ]
        }
      ],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "os.makedirs('/content/drive/MyDrive/csvs',exist_ok=True)\n",
        "for i, row in df.iterrows():\n",
        "    status_code = row['StatusCode']\n",
        "    if status_code == 200:\n",
        "        print(i+1)\n",
        "        xml_file = f'/content/drive/MyDrive/xmls/{i + 1}.xml'\n",
        "\n",
        "        try:\n",
        "            tree = ET.parse(xml_file)\n",
        "            root = tree.getroot()\n",
        "\n",
        "            images = []\n",
        "            captions = []\n",
        "            # Extracting tags for captions\n",
        "            for figure in root.findall('.//ce:figure', namespaces={'ce': 'http://www.elsevier.com/xml/common/dtd'}):\n",
        "                link_element = figure.find('.//ce:link', namespaces={'ce': 'http://www.elsevier.com/xml/common/dtd'})\n",
        "                locator = link_element.attrib.get('locator') if link_element is not None else None\n",
        "\n",
        "                caption_element = figure.find('.//ce:caption', namespaces={'ce': 'http://www.elsevier.com/xml/common/dtd'})\n",
        "                if caption_element is not None:\n",
        "                    simple_para_element = caption_element.find('.//ce:simple-para', namespaces={'ce': 'http://www.elsevier.com/xml/common/dtd'})\n",
        "                    if simple_para_element is not None:\n",
        "                        raw_caption = ET.tostring(simple_para_element, encoding='unicode', method='text')\n",
        "                        caption = re.sub('<.*?>', '', raw_caption)\n",
        "                        caption = \" \".join(caption.split())\n",
        "                    else:\n",
        "                        caption = None\n",
        "                    captions.append(caption)\n",
        "                else:\n",
        "                    captions.append(None)\n",
        "            # Extracting tags for Image\n",
        "            for attachment in root.findall('.//xocs:attachment', namespaces={'xocs': 'http://www.elsevier.com/xml/xocs/dtd'}):\n",
        "                attachment_eid = attachment.find('.//xocs:attachment-eid', namespaces={'xocs': 'http://www.elsevier.com/xml/xocs/dtd'}).text\n",
        "                ucs_locator = attachment.find('.//xocs:ucs-locator', namespaces={'xocs': 'http://www.elsevier.com/xml/xocs/dtd'}).text\n",
        "                filename = attachment.find('.//xocs:filename', namespaces={'xocs': 'http://www.elsevier.com/xml/xocs/dtd'}).text\n",
        "                images.append((attachment_eid, ucs_locator, filename))\n",
        "\n",
        "            # Adding to the csv file\n",
        "            with open(f'/content/drive/MyDrive/csvs/output{i + 1}.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "                csv_writer = csv.writer(csvfile)\n",
        "                csv_writer.writerow(['UTD EID', 'UCS Locator', 'Filename', 'Caption'])\n",
        "                for image, caption in zip(images, captions):\n",
        "                    csv_writer.writerow([image[0], image[1], image[2], caption])\n",
        "\n",
        "            df_output = pd.read_csv(f'/content/drive/MyDrive/csvs/output{i + 1}.csv')\n",
        "            df_output['link'] = df_output['UTD EID'].apply(lambda x: f'https://ars.els-cdn.com/content/image/{x}'.replace('.jpg', '_lrg.jpg'))\n",
        "            df_output.to_csv(f'/content/drive/MyDrive/csvs/output{i + 1}.csv', index=False)\n",
        "\n",
        "        except ET.ParseError as e:\n",
        "            print(f\"ParseError: {e}. The file {xml_file} might be incomplete or malformed.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while processing the file {xml_file}: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code extracts image information (including captions) from XML files, cleans the captions, creates a CSV file with the extracted information, and adds a URL column to the CSV data. It effectively processes XML data and organizes the extracted information into a structured format.\n",
        "\n"
      ],
      "metadata": {
        "id": "oH2_xVICqLX4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7P-5PL8AqaRb",
        "outputId": "7d9264d1-5096-4641-b9af-4ebaf6ad0310"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The file /content/drive/MyDrive/csvs/output38.csv does not exist.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/scopus.csv')\n",
        "\n",
        "# Keywords to be highlighted\n",
        "keywords = [\n",
        "    'strain', 'DSC', 'stress', 'heat flow', 'displacement', 'force',\n",
        "    'martensite', 'fraction', 'load', 'X-Ray', 'temp', 'Differential',\n",
        "    'scanning', 'calorimetry', 'curve'\n",
        "]\n",
        "\n",
        "# Function to highlight keywords in the caption\n",
        "def highlight_keywords(caption, keywords):\n",
        "    if not isinstance(caption, str):\n",
        "        caption = ''\n",
        "\n",
        "    found = False\n",
        "    found_keywords = []\n",
        "    for keyword in keywords:\n",
        "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', caption, re.IGNORECASE):\n",
        "            found = True\n",
        "            found_keywords.append(keyword)\n",
        "    return found, found_keywords\n",
        "\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    status_code = row['StatusCode']\n",
        "\n",
        "\n",
        "    if status_code == 200:\n",
        "        output_file = f'/content/drive/MyDrive/csvs/output{i+1}.csv'\n",
        "\n",
        "        # Check if the file exists\n",
        "        if os.path.exists(output_file):\n",
        "            try:\n",
        "                # Read the corresponding output CSV file\n",
        "                dof = pd.read_csv(output_file)\n",
        "\n",
        "                # Apply the keyword highlighting function\n",
        "                results = dof['Caption'].apply(lambda x: highlight_keywords(x, keywords))\n",
        "                dof['contains_keywords'] = results.apply(lambda x: 1 if x[0] else 0)\n",
        "                dof['found_keywords'] = results.apply(lambda x: ', '.join(x[1]) if x[1] else '')\n",
        "\n",
        "                # Save the updated DataFrame back to the CSV file\n",
        "                dof.to_csv(output_file, index=False)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while processing the file {output_file}: {e}\")\n",
        "        else:\n",
        "            print(f\"The file {output_file} does not exist.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvw4AfubmNPu"
      },
      "source": [
        "# Download and Organize Images from URLs\n",
        "\n",
        "This Python code downloads images from URLs specified in a filtered DataFrame, saves them to designated directories, and also maintains a copy in a 'Master' directory.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "* **Efficient Image Download:** Downloads images using the `requests` library with efficient streaming for large files.\n",
        "* **Directory Management:** Creates directories dynamically for organized storage and removes empty directories to maintain a clean structure.\n",
        "* **Error Handling:** Includes `try-except` blocks to handle potential exceptions during the download process, such as network errors or invalid URLs.\n",
        "* **Master Directory:** Maintains a copy of all downloaded images in a central 'Master' directory for easy access and backup.\n",
        "* **Data Filtering:** Filters the DataFrame based on a specific condition ('contains_keywords' == 1) before processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGqCqpo1sulP",
        "outputId": "26960f28-1dba-474c-abc0-9b25b995210b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-ga1_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr8_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr3_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr1_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr11_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr12_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr13_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr9_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr6_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr7_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr10_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/2/1-s2.0-S1359645424003422-gr2_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/4/1-s2.0-S2238785424013000-gr4_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/4/1-s2.0-S2238785424013000-gr3_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/4/1-s2.0-S2238785424013000-gr2_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/4/1-s2.0-S2238785424013000-gr1_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/6/1-s2.0-S1350630724006289-ga1_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/6/1-s2.0-S1350630724006289-gr1_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/6/1-s2.0-S1350630724006289-gr10_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/6/1-s2.0-S1350630724006289-gr11_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/6/1-s2.0-S1350630724006289-gr12_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/6/1-s2.0-S1350630724006289-gr8_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/6/1-s2.0-S1350630724006289-gr9_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/6/1-s2.0-S1350630724006289-gr2_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/10/1-s2.0-S0020768324002518-gr8_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/10/1-s2.0-S0020768324002518-fx1001_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/10/1-s2.0-S0020768324002518-gr16_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/10/1-s2.0-S0020768324002518-gr2_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/10/1-s2.0-S0020768324002518-gr11_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/10/1-s2.0-S0020768324002518-gr10_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/10/1-s2.0-S0020768324002518-gr12_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/10/1-s2.0-S0020768324002518-gr5_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/10/1-s2.0-S0020768324002518-gr14_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/14/1-s2.0-S0167577X24005469-ga1_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/29/1-s2.0-S2666330924000499-gr12_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/29/1-s2.0-S2666330924000499-gr1_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/29/1-s2.0-S2666330924000499-gr6_lrg.jpg\n",
            "File downloaded: /content/drive/MyDrive/images/29/1-s2.0-S2666330924000499-gr5_lrg.jpg\n",
            "Output file not found: /content/drive/MyDrive/csvs/output38.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import shutil\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/scopus.csv')\n",
        "\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    status_code = row['StatusCode']\n",
        "    mloc = f'/content/drive/MyDrive/images/Master'\n",
        "    os.makedirs(mloc,exist_ok=True)\n",
        "\n",
        "    if status_code == 200:\n",
        "        output_file = f'/content/drive/MyDrive/csvs/output{i+1}.csv'\n",
        "\n",
        "        # Check if the corresponding output CSV file exists\n",
        "        if os.path.exists(output_file):\n",
        "            dof = pd.read_csv(output_file)\n",
        "\n",
        "            # Filter the DataFrame to get rows where 'contains_keywords' is 1\n",
        "            contain = dof[dof['contains_keywords'] == 1]\n",
        "\n",
        "            # Check if there are any rows to process\n",
        "            if not contain.empty:\n",
        "                # Define the directory to save the images\n",
        "                loc = f'/content/drive/MyDrive/images/{i+1}'\n",
        "                os.makedirs(loc, exist_ok=True)\n",
        "\n",
        "                # Variable to track if any file was downloaded\n",
        "                files_downloaded = False\n",
        "\n",
        "                # Loop through the filtered rows\n",
        "                for i1, r2 in contain.iterrows():\n",
        "                    url = r2['link']\n",
        "\n",
        "                    try:\n",
        "                        response = requests.get(url, stream=True, timeout=30)\n",
        "\n",
        "                        # Check if the request was successful\n",
        "                        if response.status_code == 200:\n",
        "                            # Extract the file name from the URL\n",
        "                            file_name = url.split(\"/\")[-1]\n",
        "\n",
        "                            # Define the complete path for saving the file\n",
        "                            save_path = os.path.join(loc, file_name)\n",
        "                            mloc2 = os.path.join(mloc,file_name)\n",
        "\n",
        "                            # Write the file data to a file\n",
        "                            with open(save_path, 'wb') as file:\n",
        "                                for chunk in response.iter_content(1024):\n",
        "                                    file.write(chunk)\n",
        "                            shutil.copy2(save_path,mloc2)\n",
        "                            print(f\"File downloaded: {save_path}\")\n",
        "                            files_downloaded = True\n",
        "                        else:\n",
        "                            print(f\"Failed to download file: {url} (Status code: {response.status_code})\")\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error downloading file: {url} ({e})\")\n",
        "\n",
        "                # If no files were downloaded, remove the created directory\n",
        "                if not files_downloaded:\n",
        "                    os.rmdir(loc)\n",
        "                    print(f\"Removed empty directory: {loc}\")\n",
        "        else:\n",
        "            print(f\"Output file not found: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hFmw_abmNPv"
      },
      "source": [
        "# Installing required libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBsBRVQIEwIq",
        "outputId": "29fe0e2c-7052-4f29-dff2-ec355b0a1735"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOZRGb0nuvTl",
        "outputId": "de5d2921-4f44-4f95-df6e-8be3abbc7f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (11.0.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,632 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,564 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,448 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,226 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,517 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,517 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,830 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,614 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [33.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [81.4 kB]\n",
            "Fetched 26.9 MB in 3s (8,824 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (3,688 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libarchive-dev libleptonica-dev\n",
            "The following NEW packages will be installed:\n",
            "  libarchive-dev libleptonica-dev libtesseract-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 3,743 kB of archives.\n",
            "After this operation, 16.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libarchive-dev amd64 3.6.0-1ubuntu1.3 [581 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libleptonica-dev amd64 1.82.0-3build1 [1,562 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libtesseract-dev amd64 4.1.1-2.1build1 [1,600 kB]\n",
            "Fetched 3,743 kB in 1s (2,880 kB/s)\n",
            "Selecting previously unselected package libarchive-dev:amd64.\n",
            "(Reading database ... 123681 files and directories currently installed.)\n",
            "Preparing to unpack .../libarchive-dev_3.6.0-1ubuntu1.3_amd64.deb ...\n",
            "Unpacking libarchive-dev:amd64 (3.6.0-1ubuntu1.3) ...\n",
            "Selecting previously unselected package libleptonica-dev.\n",
            "Preparing to unpack .../libleptonica-dev_1.82.0-3build1_amd64.deb ...\n",
            "Unpacking libleptonica-dev (1.82.0-3build1) ...\n",
            "Selecting previously unselected package libtesseract-dev:amd64.\n",
            "Preparing to unpack .../libtesseract-dev_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Setting up libleptonica-dev (1.82.0-3build1) ...\n",
            "Setting up libarchive-dev:amd64 (3.6.0-1ubuntu1.3) ...\n",
            "Setting up libtesseract-dev:amd64 (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract\n",
        "!apt-get update\n",
        "!apt-get install -y tesseract-ocr\n",
        "!apt-get install -y libtesseract-dev\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm0QleTrw1vl",
        "outputId": "749e6da1-feb5-42b3-de5f-135cb740d8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tesseract 4.1.1\n",
            " leptonica-1.82.0\n",
            "  libgif 5.1.9 : libjpeg 8d (libjpeg-turbo 2.1.1) : libpng 1.6.37 : libtiff 4.3.0 : zlib 1.2.11 : libwebp 1.2.2 : libopenjp2 2.4.0\n",
            " Found AVX2\n",
            " Found AVX\n",
            " Found FMA\n",
            " Found SSE\n",
            " Found libarchive 3.6.0 zlib/1.2.11 liblzma/5.2.5 bz2lib/1.0.8 liblz4/1.9.3 libzstd/1.4.8\n"
          ]
        }
      ],
      "source": [
        "!tesseract --version\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automates the process of extracting text from images, highlighting relevant keywords, and updating a DataFrame with the extracted information.\n",
        "Functions:\n",
        "\n",
        "highlight_keywords(caption, keywords):\n",
        "\n",
        "Takes a caption (text) and a list of keywords.\n",
        "Highlights each keyword in the caption using bold tags (**keyword**).\n",
        "Returns a boolean indicating if any keywords were found and the modified caption.\n",
        "\n",
        "process_image(img_path, keywords):\n",
        "\n",
        "Takes an image path and a list of keywords.\n",
        "Opens the image, converts it to grayscale for better text extraction.\n",
        "Enhances image contrast to improve text clarity.\n",
        "Uses Tesseract OCR to extract text from the image.\n",
        "Calls highlight_keywords to highlight keywords in the extracted text.\n",
        "Returns a status code (2 if keywords found, 0 otherwise) and the extracted text.\n",
        "\n",
        "process_row(row, img_path, keywords):\n",
        "\n",
        "Takes a DataFrame row, image path, and keywords list.\n",
        "Checks if the 'contains_keywords' column value is 1 (presumably indicating the image might contain relevant keywords).\n",
        "If so, calls process_image to process the image and get results.\n",
        "Updates the row with the returned status code (contains_keywords2) and extracted text.\n",
        "Returns the updated row.\n",
        "\n",
        "main() (the main function):\n",
        "\n",
        "Reads the main DataFrame (scopus.csv).\n",
        "Iterates through each row.\n",
        "Checks if the 'StatusCode' is 200 (assuming it indicates a successful download).\n",
        "If so: Reads the corresponding output CSV file (output{i+1}.csv).\n",
        "\n",
        "Defines the image directory path based on the row index.\n",
        "Applies the process_row function to each row of the output CSV file using pandas' apply method.\n",
        "\n",
        "This function processes the image associated with each row and updates the row with extracted text and a status code.\n",
        "\n",
        "Saves the processed DataFrame back to the output CSV file.\n",
        "Handles potential errors like FileNotFoundError and generic exceptions during processing.\n",
        "Finally, saves the updated main DataFrame back to scopus.csv."
      ],
      "metadata": {
        "id": "1U00K144rLCM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODO8Ng7JuSYI",
        "outputId": "b5b0bdc7-59ab-44b6-8c1c-9ee1e2f282ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing output2.csv\n",
            "Saved processed data to output2.csv\n",
            "Processing output4.csv\n",
            "Saved processed data to output4.csv\n",
            "Processing output6.csv\n",
            "Saved processed data to output6.csv\n",
            "Processing output10.csv\n",
            "Saved processed data to output10.csv\n",
            "Processing output14.csv\n",
            "Saved processed data to output14.csv\n",
            "Processing output29.csv\n",
            "Saved processed data to output29.csv\n",
            "Error: [Errno 2] No such file or directory: '/content/drive/MyDrive/csvs/output38.csv'. Skipping processing of output38.csv\n",
            "Main data frame updated and saved.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageEnhance\n",
        "import pytesseract\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def highlight_keywords(caption, keywords):\n",
        "    \"\"\"Function to highlight keywords in a given caption.\"\"\"\n",
        "    if not isinstance(caption, str):\n",
        "        caption = ''\n",
        "\n",
        "    found = False\n",
        "    for keyword in keywords:\n",
        "        if re.search(r'\\b' + re.escape(keyword) + r'\\b', caption, re.IGNORECASE):\n",
        "            found = True\n",
        "            caption = re.sub(r'(?i)\\b' + re.escape(keyword) + r'\\b', r'**\\g<0>**', caption)\n",
        "    return found, caption\n",
        "\n",
        "def process_image(img_path, keywords):\n",
        "    \"\"\"Function to process an image, extract text, and highlight keywords.\"\"\"\n",
        "    try:\n",
        "        image = Image.open(img_path)\n",
        "        image = image.convert('L')  # Convert image to grayscale\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(2)  # Enhance image contrast\n",
        "        text = pytesseract.image_to_string(image)  # Extract text from image\n",
        "        found, highlighted_text = highlight_keywords(text, keywords)  # Highlight keywords in extracted text\n",
        "        return 2 if found else 0, text  # Return status (2 if keywords found, otherwise 0) and extracted text\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {img_path}: {e}\")\n",
        "        return 1, \"\"  # Return status 1 (error) and empty text on exception\n",
        "\n",
        "def process_row(row, img_path, keywords):\n",
        "    \"\"\"Function to process a row of data.\"\"\"\n",
        "    if row['contains_keywords'] == 1:\n",
        "        contains_keywords2, text = process_image(img_path, keywords)  # Process image and get results\n",
        "        row['contains_keywords2'] = contains_keywords2  # Store result in 'contains_keywords2' column\n",
        "        row['text'] = text  # Store extracted text in 'text' column\n",
        "    return row  # Return processed row\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute processing on CSV files.\"\"\"\n",
        "    df = pd.read_csv('/content/drive/MyDrive/scopus.csv')\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        if row['StatusCode'] == 200:  # Check if StatusCode is 200(file is present)\n",
        "            try:\n",
        "                # Read corresponding output CSV file\n",
        "                dof = pd.read_csv(f'/content/drive/MyDrive/csvs/output{i+1}.csv')\n",
        "                print(f\"Processing output{i+1}.csv\")\n",
        "\n",
        "                img_dir = f\"/content/drive/MyDrive/images/{i+1}/\"  # Directory path for images\n",
        "                # Apply processing to each row of the output CSV file\n",
        "                dof = dof.apply(\n",
        "                    lambda dof_row: process_row(dof_row, os.path.join(img_dir, dof_row['UTD EID'][:-4] + '_lrg.jpg'), keywords),\n",
        "                    axis=1\n",
        "                )\n",
        "\n",
        "                dof.to_csv(f'/content/drive/MyDrive/csvs/output{i+1}.csv', index=False)  # Save processed data back to output CSV file\n",
        "                print(f\"Saved processed data to output{i+1}.csv\")\n",
        "\n",
        "            except FileNotFoundError as fnf_error:\n",
        "                print(f\"Error: {fnf_error}. Skipping processing of output{i+1}.csv\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error processing output{i+1}.csv: {e}\")\n",
        "\n",
        "    df.to_csv('/content/drive/MyDrive/scopus.csv', index=False)  # Save updated main DataFrame back to scopus2.csv\n",
        "    print(\"Main data frame updated and saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Keywords to consider for highlighting\n",
        "    keywords = [\n",
        "        'strain', 'DSC', 'stress', 'heat flow', 'displacement', 'force',\n",
        "        'martensite', 'fraction', 'load', 'X-Ray', 'temp', 'Differential',\n",
        "        'scanning', 'calorimetry', 'curve'\n",
        "    ]\n",
        "    main()  # Execute main function if script is run directly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0Q_hl1w7YoZ"
      },
      "source": [
        "# Creating Individual Files for each paper with Relevant Images , CSV file ,\n",
        "\n",
        "###Refined Image Directory\n",
        "\n",
        "Creates a separate directory for images containing relevant keywords, improving organization and easier access.\n",
        "\n",
        "###Data File Copying\n",
        "\n",
        "Copies the corresponding XML file and output CSV file along with the image, ensuring all relevant data is grouped together.\n",
        "\n",
        "### Link Saving\n",
        "\n",
        "Saves the original image link to a text file within the refined image directory for reference.\n",
        "\n",
        "XMLS file and the Link of the Paper\n",
        "\n",
        "Read DataFrame and Initialize Variables:\n",
        "\n",
        "Reads the scopus.csv DataFrame.\n",
        "Initializes a counter (count) to track the number of images copied.\n",
        "Sets the 'Processed' column value to 0 for each row initially.\n",
        "Iterate Through DataFrame Rows:\n",
        "\n",
        "Loops through each row in the DataFrame.\n",
        "Process Rows with Successful Download (Status Code 200):\n",
        "\n",
        "Checks if the 'StatusCode' is 200 (indicating a successful download).\n",
        "If yes: Attempts to read the corresponding output CSV file (output{i+1}.csv).\n",
        "Prints a message indicating processing of the current output file.\n",
        "Defines paths for the image directory (img_dir) and the refined image directory (refined_img_dir).\n",
        "\n",
        "Checks if there are rows in the output CSV where contains_keywords2 is 2 (presumably indicating relevant keywords found).\n",
        "If there are matching rows: Creates the refined_img_dir directory (if it doesn't exist).\n",
        "\n",
        "Initialize a flag (copied) to track if any image was copied.\n",
        "Loops through each row in the output CSV file.\n",
        "\n",
        "Attempt to copy images where contains_keywords2 is 2:\n",
        "Increments the counter (count) for each copied image.\n",
        "Constructs source and destination image paths based on the row's UTD EID value.\n",
        "Copies the image using shutil.copy.\n",
        "Sets the copied flag to True.\n",
        "\n",
        "Check if any image was copied:\n",
        "If images were copied:\n",
        "Copy the corresponding XML file (firstloc) and output CSV file (secondloc) to the refined_img_dir.\n",
        "Update the 'Processed' column value in the main DataFrame (scopus.csv) to 1 for the current row.\n",
        "Save the link from the original row to a text file named Link_{i+1}.txt within the refined_img_dir.\n",
        "Save the updated output CSV file (dof) back to its original location.\n",
        "\n",
        "Error Handling:\n",
        "\n",
        "Handle FileNotFoundError if the corresponding output CSV file is not found.\n",
        "Catche generic exceptions during processing and prints an error message.\n",
        "Print Summary and Save DataFrame:\n",
        "\n",
        "Print the total number of images copied.\n",
        "Save the updated scopus.csv DataFrame with the 'Processed' column information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "270DW9rEwhxq",
        "outputId": "bba2061f-a81f-4a7d-a09f-21e217e3bea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing output2.csv\n",
            "Link saved to /content/drive/MyDrive/images/Refined images/2/Link_2.txt\n",
            "Processing output4.csv\n",
            "Link saved to /content/drive/MyDrive/images/Refined images/4/Link_4.txt\n",
            "Processing output6.csv\n",
            "Link saved to /content/drive/MyDrive/images/Refined images/6/Link_6.txt\n",
            "Processing output10.csv\n",
            "Link saved to /content/drive/MyDrive/images/Refined images/10/Link_10.txt\n",
            "Processing output14.csv\n",
            "Processing output29.csv\n",
            "Link saved to /content/drive/MyDrive/images/Refined images/29/Link_29.txt\n",
            "FileNotFoundError: output38.csv not found.\n",
            "Total images copied: 17\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Function to process the images and CSV files\n",
        "def main():\n",
        "    df = pd.read_csv('/content/drive/MyDrive/scopus.csv')\n",
        "    count = 0\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        df.at[i, 'Processed'] = 0\n",
        "        if row['StatusCode'] == 200:\n",
        "            try:\n",
        "                dof = pd.read_csv(f'/content/drive/MyDrive/csvs/output{i+1}.csv')\n",
        "                print(f\"Processing output{i+1}.csv\")\n",
        "\n",
        "                img_dir = f\"/content/drive/MyDrive/images/{i+1}/\"\n",
        "                refined_img_dir = f\"/content/drive/MyDrive/images/Refined images/{i+1}/\"\n",
        "\n",
        "                # Check if there are rows in dof that meet the condition\n",
        "                if len(dof) > 0 and any(dof['contains_keywords2'] == 2):\n",
        "                    os.makedirs(refined_img_dir, exist_ok=True)\n",
        "\n",
        "                    copied = False\n",
        "                    for index, r1 in dof.iterrows():\n",
        "                        try:\n",
        "                            if r1['contains_keywords2'] == 2:\n",
        "                                count += 1\n",
        "                                src_img = os.path.join(img_dir, r1['UTD EID'][:-4] + '_lrg.jpg')\n",
        "                                dst_img = os.path.join(refined_img_dir, r1['UTD EID'][:-4] + '_lrg.jpg')\n",
        "                                shutil.copy(src_img, dst_img)\n",
        "                                copied = True\n",
        "                        except Exception as e:\n",
        "                            print(f\"Error processing row {index}: {e}\")\n",
        "\n",
        "                    # Check if any image was copied\n",
        "                    if copied:\n",
        "                        firstloc = f\"/content/drive/MyDrive/xmls/{i+1}.xml\"\n",
        "                        secondloc = f\"/content/drive/MyDrive/csvs/output{i+1}.csv\"\n",
        "                        shutil.copy(firstloc, refined_img_dir)\n",
        "                        shutil.copy(secondloc, refined_img_dir)\n",
        "\n",
        "                        # Update scopus.csv to mark this row as processed\n",
        "                        df.at[i, 'Processed'] = 1\n",
        "\n",
        "                        # Save the link to a text file in the refined image directory\n",
        "                        link = row['Link']\n",
        "                        link_file_path = os.path.join(refined_img_dir, f\"Link_{i+1}.txt\")\n",
        "                        with open(link_file_path, 'w') as link_file:\n",
        "                            link_file.write(link)\n",
        "                            print(f\"Link saved to {link_file_path}\")\n",
        "\n",
        "                dof.to_csv(f'/content/drive/MyDrive/csvs/output{i+1}.csv', index=False)\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                print(f\"FileNotFoundError: output{i+1}.csv not found.\")\n",
        "            except Exception as ex:\n",
        "                print(f\"Error processing output{i+1}.csv: {ex}\")\n",
        "\n",
        "    print(f\"Total images copied: {count}\")\n",
        "\n",
        "    # Save the updated scopus.csv with the 'Processed' column\n",
        "    df.to_csv('/content/drive/MyDrive/scopus.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFPo_MBL0afX",
        "outputId": "409e7805-d4b4-46ca-fcc9-bb822141c9c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.52-py3-none-any.whl.metadata (35 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.8.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.13-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.52-py3-none-any.whl (901 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m901.7/901.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.13-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.52 ultralytics-thop-2.0.13\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g99kWtIwASUT",
        "outputId": "ec6b9635-aad6-4521-aff2-4faa45419ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "Cloning into 'sma_train'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 34 (delta 0), reused 31 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (34/34), 17.14 MiB | 12.29 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive\n",
        "!git clone https://github.com/PrathameshLadhe/sma_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FecbtvWw2vZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350033f7-250c-4367-843e-244324e1b4c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file  \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "Ultralytics 8.3.52  Python-3.10.12 torch-2.5.1+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Model summary (fused): 168 layers, 3,006,233 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\n",
            "image 1/38 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-fx1001_lrg.jpg: 640x448 (no detections), 327.5ms\n",
            "image 2/38 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr10_lrg.jpg: 352x640 (no detections), 217.2ms\n",
            "image 3/38 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr11_lrg.jpg: 288x640 2 graphs, 2 xlabels, 3 ylabels, 177.5ms\n",
            "image 4/38 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr12_lrg.jpg: 224x640 2 graphs, 2 xlabels, 3 ylabels, 135.8ms\n",
            "image 5/38 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr14_lrg.jpg: 480x640 (no detections), 248.8ms\n",
            "image 6/38 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr16_lrg.jpg: 384x640 3 graphs, 4 xlabels, 4 ylabels, 233.3ms\n",
            "image 7/38 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr2_lrg.jpg: 608x640 1 graph, 343.3ms\n",
            "image 8/38 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr5_lrg.jpg: 480x640 1 graph, 1 xlabel, 2 ylabels, 257.4ms\n",
            "image 9/38 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr8_lrg.jpg: 192x640 2 graphs, 3 xlabels, 3 ylabels, 120.2ms\n",
            "image 10/38 /content/drive/MyDrive/images/Master/1-s2.0-S0167577X24005469-ga1_lrg.jpg: 352x640 1 graph, 1 xlabel, 1 ylabel, 204.0ms\n",
            "image 11/38 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-ga1_lrg.jpg: 256x640 (no detections), 147.5ms\n",
            "image 12/38 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr10_lrg.jpg: 448x640 (no detections), 272.2ms\n",
            "image 13/38 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr11_lrg.jpg: 416x640 (no detections), 244.2ms\n",
            "image 14/38 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr12_lrg.jpg: 480x640 (no detections), 265.5ms\n",
            "image 15/38 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr1_lrg.jpg: 256x640 (no detections), 154.6ms\n",
            "image 16/38 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr2_lrg.jpg: 640x608 5 graphs, 5 xlabels, 7 ylabels, 341.5ms\n",
            "image 17/38 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr8_lrg.jpg: 512x640 (no detections), 199.2ms\n",
            "image 18/38 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr9_lrg.jpg: 640x448 (no detections), 151.7ms\n",
            "image 19/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-ga1_lrg.jpg: 576x640 1 graph, 1 xlabel, 1 ylabel, 197.9ms\n",
            "image 20/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr10_lrg.jpg: 480x640 1 graph, 161.5ms\n",
            "image 21/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr11_lrg.jpg: 640x480 (no detections), 173.7ms\n",
            "image 22/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr12_lrg.jpg: 512x640 (no detections), 171.9ms\n",
            "image 23/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr13_lrg.jpg: 640x544 1 graph, 190.3ms\n",
            "image 24/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr1_lrg.jpg: 256x640 1 graph, 2 xlabels, 1 ylabel, 96.0ms\n",
            "image 25/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr2_lrg.jpg: 640x448 2 graphs, 2 xlabels, 2 ylabels, 176.1ms\n",
            "image 26/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr3_lrg.jpg: 640x480 1 graph, 1 xlabel, 2 ylabels, 160.9ms\n",
            "image 27/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr6_lrg.jpg: 640x480 1 graph, 1 xlabel, 1 ylabel, 176.5ms\n",
            "image 28/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr7_lrg.jpg: 640x640 1 graph, 1 xlabel, 1 ylabel, 222.6ms\n",
            "image 29/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr8_lrg.jpg: 640x448 2 graphs, 2 xlabels, 2 ylabels, 179.4ms\n",
            "image 30/38 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr9_lrg.jpg: 640x544 (no detections), 293.8ms\n",
            "image 31/38 /content/drive/MyDrive/images/Master/1-s2.0-S2238785424013000-gr1_lrg.jpg: 352x640 4 graphs, 3 xlabels, 3 ylabels, 204.2ms\n",
            "image 32/38 /content/drive/MyDrive/images/Master/1-s2.0-S2238785424013000-gr2_lrg.jpg: 512x640 4 graphs, 4 xlabels, 6 ylabels, 209.0ms\n",
            "image 33/38 /content/drive/MyDrive/images/Master/1-s2.0-S2238785424013000-gr3_lrg.jpg: 640x640 3 ylabels, 208.6ms\n",
            "image 34/38 /content/drive/MyDrive/images/Master/1-s2.0-S2238785424013000-gr4_lrg.jpg: 512x640 1 graph, 1 xlabel, 1 ylabel, 180.5ms\n",
            "image 35/38 /content/drive/MyDrive/images/Master/1-s2.0-S2666330924000499-gr12_lrg.jpg: 544x640 1 graph, 1 xlabel, 1 ylabel, 190.0ms\n",
            "image 36/38 /content/drive/MyDrive/images/Master/1-s2.0-S2666330924000499-gr1_lrg.jpg: 352x640 (no detections), 121.3ms\n",
            "image 37/38 /content/drive/MyDrive/images/Master/1-s2.0-S2666330924000499-gr5_lrg.jpg: 544x640 1 graph, 1 xlabel, 1 ylabel, 190.2ms\n",
            "image 38/38 /content/drive/MyDrive/images/Master/1-s2.0-S2666330924000499-gr6_lrg.jpg: 512x640 1 graph, 1 xlabel, 1 ylabel, 210.5ms\n",
            "Speed: 4.6ms preprocess, 204.1ms inference, 1.7ms postprocess per image at shape (1, 3, 512, 640)\n",
            "Results saved to \u001b[1mruns/detect/predict\u001b[0m\n",
            " Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ],
      "source": [
        "## Running a model on images with pre-trained weights to detect plots , their x-label and y-label\n",
        "!yolo task=detect mode=predict model='/content/drive/MyDrive/sma_train/train/weights/best.pt' conf=0.25 source='/content/drive/MyDrive/images/Master'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving all the images in the Drive\n",
        "\n",
        "Code effectively archives a directory (runs) located in Google Drive.\n",
        "\n",
        "Define Paths:\n",
        "\n",
        "src_dir: Path to the source directory containing files to be archived (/content/drive/MyDrive/runs).\n",
        "base_dst_dir: Base path for the archived directory (/content/drive/MyDrive/Archived_Runs).\n",
        "timestamp: Current date and time formatted for creating a unique directory name (e.g., 20241221_162820).\n",
        "dst_dir: Complete destination directory path with the timestamp incorporated (/content/drive/MyDrive/Archived_Runs/runs_20241221_162820).\n",
        "\n",
        "\n",
        "Create Destination Directory:\n",
        "\n",
        "os.makedirs(base_dst_dir, exist_ok=True)\n",
        "\n",
        "ensures the Archived_Runs directory exists (if not already created) to avoid errors during archiving.\n",
        "\n",
        "Copy Directory:\n",
        "\n",
        "shutil.copytree(src_dir, dst_dir)\n",
        "\n",
        "attempts to copy the entire contents of the source directory (runs) to the destination directory (runs_{timestamp}).\n",
        "\n",
        "Print Success Message:\n",
        "\n",
        "If copying is successful, it prints a confirmation message indicating the source and destination paths.\n",
        "\n",
        "Delete Source Directory (Optional):\n",
        "\n",
        "shutil.rmtree('/content/drive/MyDrive/runs')\n",
        "\n",
        "permanently deletes the source directory (runs) after successful archiving. This is an optional step depending on your preference for keeping or removing the source files.\n",
        "\n",
        "Error Handling:\n",
        "\n",
        "The try-except block ensures graceful handling of potential exceptions during the copying process.\n",
        "If an error occurs (Exception as e), it prints an error message with the exception details."
      ],
      "metadata": {
        "id": "Nqa_v6QCt731"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUM5d0NI3cQL",
        "outputId": "4a952b99-a943-48d2-aa37-3bf24d5806e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder copied from /content/drive/MyDrive/runs to /content/drive/MyDrive/Archived_Runs/runs_20241221_145753\n",
            "File deleted.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "from datetime import datetime\n",
        "# Define source and destination paths\n",
        "src_dir = '/content/drive/MyDrive/runs'\n",
        "base_dst_dir = '/content/drive/MyDrive/Archived_Runs'\n",
        "\n",
        "# Create a timestamped folder name to avoid overwriting\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "dst_dir = os.path.join(base_dst_dir, f'runs_{timestamp}')\n",
        "\n",
        "# Ensure the base destination directory exists\n",
        "os.makedirs(base_dst_dir, exist_ok=True)\n",
        "\n",
        "# Copy the entire directory\n",
        "try:\n",
        "    shutil.copytree(src_dir, dst_dir)\n",
        "    print(f\"Folder copied from {src_dir} to {dst_dir}\")\n",
        "    shutil.rmtree('/content/drive/MyDrive/runs')\n",
        "    print('File deleted.')\n",
        "except Exception as e:\n",
        "    print(f\"Error copying folder: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the YOLO Predictions in YOLO txt format for further use.\n",
        "\n",
        "Utilize the Ultralytics YOLO library for object detection on a set of images. Here's a breakdown of the steps involved:\n",
        "\n",
        "1. Load YOLO Model:\n",
        "\n",
        "Import the YOLO class from the ultralytics library.\n",
        "Load a pre-trained YOLO model from the specified path (/content/drive/MyDrive/sma_train/train/weights/best.pt). This model is trained on custom dataset for object detection.\n",
        "\n",
        "2. Create Output Directory:\n",
        "\n",
        "Define the output directory path (output_dir) where the detection results will be saved as text files.\n",
        "Uses os.makedirs(output_dir, exist_ok=True) to create the output directory if it doesn't already exist.\n",
        "\n",
        "3. Iterate Over Images:\n",
        "\n",
        "Define the image directory path (image_dir) containing the images for which object detection needs to be performed.\n",
        "Loops through all files in the image_dir.\n",
        "Filters for files with image extensions (.jpg, .png, .jpeg) using if image_file.endswith(...).\n",
        "\n",
        "4. Process Each Image:\n",
        "\n",
        "Constructs the full path to the current image file (image_path).\n",
        "Use the loaded YOLO model (model) to perform object detection on the image.\n",
        "The model(image_path, save_txt=None) call infers objects in the image and returns the detections.\n",
        "The save_txt=None argument prevents saving detections as images by default (you might want to adjust this based on your needs).\n",
        "Store the returned detections in the predictions variable.\n",
        "\n",
        "\n",
        "5. Prepare Output File:\n",
        "\n",
        "Extract the filename without the extension from the current image file (base_name).Construct the output file path (output_file_path) within the output_dir using the extracted filename and appending a .txt extension.\n",
        "\n",
        "6. Save Detections to Text File:\n",
        "\n",
        "Open the output text file (output_file_path) for writing.\n",
        "Iterate through each detected bounding box in the predictions.\n",
        "Extract the class ID (cls) for the detected object.\n",
        "Extract the bounding box coordinates (x_center, y_center, width, height) from the current box.\n",
        "Write a line to the text file in the YOLO label format: {class_id} {x_center} {y_center} {width} {height}. This format is commonly used for object detection datasets."
      ],
      "metadata": {
        "id": "uoSXOU8TvA84"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKobXp8T4Z9j",
        "outputId": "960a509f-63dd-4fcd-be98-cde6580498d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-ga1_lrg.jpg: 576x640 1 graph, 1 xlabel, 1 ylabel, 242.7ms\n",
            "Speed: 11.3ms preprocess, 242.7ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr8_lrg.jpg: 640x448 2 graphs, 2 xlabels, 2 ylabels, 171.0ms\n",
            "Speed: 3.7ms preprocess, 171.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr3_lrg.jpg: 640x480 1 graph, 1 xlabel, 2 ylabels, 184.6ms\n",
            "Speed: 3.9ms preprocess, 184.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr1_lrg.jpg: 256x640 1 graph, 2 xlabels, 1 ylabel, 108.7ms\n",
            "Speed: 2.4ms preprocess, 108.7ms inference, 1.0ms postprocess per image at shape (1, 3, 256, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr11_lrg.jpg: 640x480 (no detections), 173.5ms\n",
            "Speed: 4.1ms preprocess, 173.5ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr12_lrg.jpg: 512x640 (no detections), 185.0ms\n",
            "Speed: 3.7ms preprocess, 185.0ms inference, 0.6ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr13_lrg.jpg: 640x544 1 graph, 240.7ms\n",
            "Speed: 3.9ms preprocess, 240.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr9_lrg.jpg: 640x544 (no detections), 201.2ms\n",
            "Speed: 4.3ms preprocess, 201.2ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 544)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr6_lrg.jpg: 640x480 1 graph, 1 xlabel, 1 ylabel, 181.5ms\n",
            "Speed: 4.0ms preprocess, 181.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr7_lrg.jpg: 640x640 1 graph, 1 xlabel, 1 ylabel, 308.1ms\n",
            "Speed: 4.8ms preprocess, 308.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr10_lrg.jpg: 480x640 1 graph, 278.9ms\n",
            "Speed: 6.7ms preprocess, 278.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1359645424003422-gr2_lrg.jpg: 640x448 2 graphs, 2 xlabels, 2 ylabels, 270.4ms\n",
            "Speed: 5.4ms preprocess, 270.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S2238785424013000-gr4_lrg.jpg: 512x640 1 graph, 1 xlabel, 1 ylabel, 230.3ms\n",
            "Speed: 5.4ms preprocess, 230.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S2238785424013000-gr3_lrg.jpg: 640x640 3 ylabels, 218.7ms\n",
            "Speed: 4.9ms preprocess, 218.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S2238785424013000-gr2_lrg.jpg: 512x640 4 graphs, 4 xlabels, 6 ylabels, 187.1ms\n",
            "Speed: 3.7ms preprocess, 187.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S2238785424013000-gr1_lrg.jpg: 352x640 4 graphs, 3 xlabels, 3 ylabels, 207.4ms\n",
            "Speed: 3.9ms preprocess, 207.4ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-ga1_lrg.jpg: 256x640 (no detections), 154.2ms\n",
            "Speed: 3.0ms preprocess, 154.2ms inference, 0.7ms postprocess per image at shape (1, 3, 256, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr1_lrg.jpg: 256x640 (no detections), 142.6ms\n",
            "Speed: 3.1ms preprocess, 142.6ms inference, 0.7ms postprocess per image at shape (1, 3, 256, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr10_lrg.jpg: 448x640 (no detections), 252.5ms\n",
            "Speed: 4.8ms preprocess, 252.5ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr11_lrg.jpg: 416x640 (no detections), 226.4ms\n",
            "Speed: 4.2ms preprocess, 226.4ms inference, 0.7ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr12_lrg.jpg: 480x640 (no detections), 258.4ms\n",
            "Speed: 5.3ms preprocess, 258.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr8_lrg.jpg: 512x640 (no detections), 273.0ms\n",
            "Speed: 5.5ms preprocess, 273.0ms inference, 0.8ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr9_lrg.jpg: 640x448 (no detections), 239.8ms\n",
            "Speed: 4.9ms preprocess, 239.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S1350630724006289-gr2_lrg.jpg: 640x608 5 graphs, 5 xlabels, 7 ylabels, 329.9ms\n",
            "Speed: 6.4ms preprocess, 329.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 608)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr8_lrg.jpg: 192x640 2 graphs, 3 xlabels, 3 ylabels, 117.3ms\n",
            "Speed: 2.5ms preprocess, 117.3ms inference, 1.1ms postprocess per image at shape (1, 3, 192, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-fx1001_lrg.jpg: 640x448 (no detections), 234.0ms\n",
            "Speed: 5.5ms preprocess, 234.0ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr16_lrg.jpg: 384x640 3 graphs, 4 xlabels, 4 ylabels, 215.7ms\n",
            "Speed: 4.4ms preprocess, 215.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr2_lrg.jpg: 608x640 1 graph, 340.9ms\n",
            "Speed: 5.9ms preprocess, 340.9ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr11_lrg.jpg: 288x640 2 graphs, 2 xlabels, 3 ylabels, 178.6ms\n",
            "Speed: 3.1ms preprocess, 178.6ms inference, 1.3ms postprocess per image at shape (1, 3, 288, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr10_lrg.jpg: 352x640 (no detections), 208.3ms\n",
            "Speed: 4.0ms preprocess, 208.3ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr12_lrg.jpg: 224x640 2 graphs, 2 xlabels, 3 ylabels, 156.5ms\n",
            "Speed: 2.6ms preprocess, 156.5ms inference, 1.3ms postprocess per image at shape (1, 3, 224, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr5_lrg.jpg: 480x640 1 graph, 1 xlabel, 2 ylabels, 253.3ms\n",
            "Speed: 5.2ms preprocess, 253.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0020768324002518-gr14_lrg.jpg: 480x640 (no detections), 169.5ms\n",
            "Speed: 4.9ms preprocess, 169.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S0167577X24005469-ga1_lrg.jpg: 352x640 1 graph, 1 xlabel, 1 ylabel, 122.4ms\n",
            "Speed: 2.6ms preprocess, 122.4ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S2666330924000499-gr12_lrg.jpg: 544x640 1 graph, 1 xlabel, 1 ylabel, 192.6ms\n",
            "Speed: 3.8ms preprocess, 192.6ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S2666330924000499-gr1_lrg.jpg: 352x640 (no detections), 121.0ms\n",
            "Speed: 2.6ms preprocess, 121.0ms inference, 0.5ms postprocess per image at shape (1, 3, 352, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S2666330924000499-gr6_lrg.jpg: 512x640 1 graph, 1 xlabel, 1 ylabel, 195.9ms\n",
            "Speed: 3.7ms preprocess, 195.9ms inference, 1.0ms postprocess per image at shape (1, 3, 512, 640)\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/images/Master/1-s2.0-S2666330924000499-gr5_lrg.jpg: 544x640 1 graph, 1 xlabel, 1 ylabel, 186.2ms\n",
            "Speed: 3.8ms preprocess, 186.2ms inference, 1.0ms postprocess per image at shape (1, 3, 544, 640)\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "\n",
        "# Load the model\n",
        "model = YOLO('/content/drive/MyDrive/sma_train/train/weights/best.pt')\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "output_dir = '/content/drive/MyDrive/images/Master/output'  # Keep this for saving results\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Iterate over images in the CORRECT directory\n",
        "image_dir = '/content/drive/MyDrive/images/Master'  # This is where the images should be\n",
        "for image_file in os.listdir(image_dir):\n",
        "    if image_file.endswith(('.jpg', '.png', '.jpeg')):  # Filter for image files\n",
        "        # Full path to the image\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "\n",
        "        # Get predictions for the image\n",
        "        predictions = model(image_path, save_txt=None)\n",
        "\n",
        "        # Prepare output file path\n",
        "        base_name = os.path.splitext(image_file)[0]\n",
        "        output_file_path = os.path.join(output_dir, f\"{base_name}.txt\")\n",
        "\n",
        "        # Write predictions to the text file\n",
        "        with open(output_file_path, 'w') as file:\n",
        "            for idx, box in enumerate(predictions[0].boxes.xywhn):  # Iterate over each detected box\n",
        "                cls = int(predictions[0].boxes.cls[idx].item())\n",
        "                # Write line to file in YOLO label format: cls x_center y_center width height\n",
        "                file.write(f\"{cls} {box[0].item()} {box[1].item()} {box[2].item()} {box[3].item()}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NU39Pxj4wstk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}